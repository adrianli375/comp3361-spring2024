{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.txt** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **1. clean all outputs and 2. run all cells in your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJN8sZsW-LnQ"
      },
      "source": [
        "# Installation of libraries/packages if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MEsbguq-LnR",
        "outputId": "2b078a56-4d69-4df1-f982-33828718230a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim==4.3.2 in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (6.4.0)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (4.66.2)\n",
            "Collecting scikit-learn==1.3.0\n",
            "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (3.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.3.0\n"
          ]
        }
      ],
      "source": [
        "%pip install gensim==4.3.2\n",
        "%pip install nltk==3.8.1\n",
        "%pip install scikit-learn==1.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt8NNllZ-LnT"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KyLbcv9u-LnU"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from nltk.lm import Vocabulary, MLE, Laplace, Lidstone\n",
        "from nltk.lm.api import LanguageModel\n",
        "from nltk.lm.counter import NgramCounter\n",
        "from nltk.util import bigrams, ngrams, trigrams\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from typing import List, Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VfEhCmIU-LnV"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 3361\n",
        "MAX_ITER = 1000\n",
        "UID = '<YOUR UID HERE>'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "# 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsSAtTqt7Q8a",
        "outputId": "7317b8e0-6013-4978-c682-6854fb2ac0af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-17 09:31:42--  https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8385238 (8.0M) [text/plain]\n",
            "Saving to: ‘data/lm/train.txt’\n",
            "\n",
            "data/lm/train.txt   100%[===================>]   8.00M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-02-17 09:31:42 (90.9 MB/s) - ‘data/lm/train.txt’ saved [8385238/8385238]\n",
            "\n",
            "--2024-02-17 09:31:42--  https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1680641 (1.6M) [text/plain]\n",
            "Saving to: ‘data/lm/dev.txt’\n",
            "\n",
            "data/lm/dev.txt     100%[===================>]   1.60M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-02-17 09:31:42 (29.2 MB/s) - ‘data/lm/dev.txt’ saved [1680641/1680641]\n",
            "\n",
            "--2024-02-17 09:31:42--  https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1670316 (1.6M) [text/plain]\n",
            "Saving to: ‘data/lm/test.txt’\n",
            "\n",
            "data/lm/test.txt    100%[===================>]   1.59M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-02-17 09:31:42 (26.1 MB/s) - ‘data/lm/test.txt’ saved [1670316/1670316]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/lm\n",
        "!wget -O data/lm/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O data/lm/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O data/lm/test.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "## 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q-rNT_QL8Dvt"
      },
      "outputs": [],
      "source": [
        "# read the train.txt file and append to the list train_corpus\n",
        "with open('data/lm/train.txt', 'r') as file:\n",
        "    sentences_training = file.readlines()\n",
        "    for i, text in enumerate(sentences_training):\n",
        "        text_list = text.split()\n",
        "        text_list.insert(0, '<START>')\n",
        "        text_list.append('<END>')\n",
        "        sentences_training[i] = text_list\n",
        "words = [word for sublist in sentences_training for word in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2xqBYCZ-LnZ",
        "outputId": "600d624a-5329-4e22-ee09-c3846e8cd4fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-02-17 09:31:43.941808 Vocabulary length = 26601\n"
          ]
        }
      ],
      "source": [
        "# this vocab variable includes the words inside train.txt and <UNK>\n",
        "vocab = Vocabulary(words, unk_cutoff=3)\n",
        "print(datetime.now(), f'Vocabulary length = {len(vocab)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "For the n-gram models, the number of parameters is of order $O(|V|^n)$, which grows rapidly under a polynomial relationship when the vocabulary size $|V|$ is large.\n",
        "\n",
        "Specifically, for the case of this bigram, the vocabulary size $|V|$ is $26601$. Excluding the special tokens `<START>` and `<END>`, the vocabulary size is $26599$, which there are a total of $26599^2 = 707506801$ possible combinations of these words inside te vocabulary. Including the combinations of the `<START> *` and the `* <END>`, there are a total of $26599^2 + 26599 * 2 = 707559999$ parameters involved for each sentence, each representing a bigram from any two combinations of the vocabulary words and the `<START>` and `<END>` tokens.\n",
        "\n",
        "From the example above, it is clear that the number of parameters inside the n-gram models suffer from the curse of dimensionality. As the dimension $n$ increases, the number of features inside a sentence grows rapidly, which makes the training data sparse. The computational power needed to perform subsequent tasks in training the n-gram model takes significantly more time as the vocabulary size increases, since the time complexity is $O(|V|^n)$ for operations involving all parameters and features of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "## 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "### Code for Bigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UoUSOMO8-Lnc"
      },
      "outputs": [],
      "source": [
        "# read the dev.txt file\n",
        "with open('data/lm/dev.txt', 'r') as file:\n",
        "    sentences_dev = file.readlines()\n",
        "    for i, text in enumerate(sentences_dev):\n",
        "        text_list = text.split()\n",
        "        text_list.insert(0, '<START>')\n",
        "        text_list.append('<END>')\n",
        "        sentences_dev[i] = text_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACSfNZGE8Yw2",
        "outputId": "56eb480c-4953-401c-efdf-0994c710dc68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-02-17 09:31:47.927439 training bigram model...\n",
            "Bigram perplexity on training set = 76.93455019656903\n",
            "Bigram perplexity on dev set = inf\n"
          ]
        }
      ],
      "source": [
        "# bigram model - training set\n",
        "bigram_text_training = []\n",
        "training_bigrams = []\n",
        "for sentence in sentences_training:\n",
        "    sentence = list(vocab.lookup(sentence))\n",
        "    bigram_pairs = list(bigrams(sentence))\n",
        "    bigram_text_training.append(bigram_pairs)\n",
        "    for bigram in bigram_pairs:\n",
        "        training_bigrams.append(bigram)\n",
        "bigram_model = MLE(order=2, vocabulary=vocab)\n",
        "print(datetime.now(), 'training bigram model...')\n",
        "bigram_model.fit(bigram_text_training, vocab)\n",
        "training_perplexity = bigram_model.perplexity(training_bigrams)\n",
        "print(f'Bigram perplexity on training set = {training_perplexity}')\n",
        "# bigram model - development set\n",
        "dev_bigrams = []\n",
        "for sentence in sentences_dev:\n",
        "    sentence = list(vocab.lookup(sentence))\n",
        "    bigram_pairs = list(bigrams(sentence))\n",
        "    for bigram in bigram_pairs:\n",
        "        dev_bigrams.append(bigram)\n",
        "dev_perplexity = bigram_model.perplexity(dev_bigrams)\n",
        "print(f'Bigram perplexity on dev set = {dev_perplexity}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xax-VPJG-Lnd"
      },
      "source": [
        "### Code for Unigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1RX-Tgi-Lne",
        "outputId": "9be2663f-2b23-4cd5-e000-b6777d13c8ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-02-17 09:32:33.071718 training unigram model...\n",
            "Unigram perplexity on training set = 888.8280671470116\n",
            "Unigram perplexity on dev set = 815.9305409683114\n"
          ]
        }
      ],
      "source": [
        "# unigram model - training set\n",
        "unigram_text_training = []\n",
        "training_unigrams = []\n",
        "for sentence in sentences_training:\n",
        "    sentence = list(vocab.lookup(sentence))\n",
        "    unigrams = list(ngrams(sentence, n=1))\n",
        "    unigram_text_training.append(unigrams)\n",
        "    for unigram in unigrams:\n",
        "        training_unigrams.append(unigram)\n",
        "unigram_model = MLE(order=1, vocabulary=vocab)\n",
        "print(datetime.now(), 'training unigram model...')\n",
        "unigram_model.fit(unigram_text_training, vocab)\n",
        "training_perplexity = unigram_model.perplexity(training_unigrams)\n",
        "print(f'Unigram perplexity on training set = {training_perplexity}')\n",
        "# unigram model - development set\n",
        "dev_unigrams = []\n",
        "for sentence in sentences_dev:\n",
        "    sentence = list(vocab.lookup(sentence))\n",
        "    unigrams = list(ngrams(sentence, n=1))\n",
        "    for unigram in unigrams:\n",
        "        dev_unigrams.append(unigram)\n",
        "dev_perplexity = unigram_model.perplexity(dev_unigrams)\n",
        "print(f'Unigram perplexity on dev set = {dev_perplexity}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4gcgL--Ylh"
      },
      "source": [
        "For the bigram language model, the perplexity on the training set is $76.9$ while the perplexity on the dev set is `infinity`.\n",
        "\n",
        "For the unigram language model, the perplexity on the training set is $888.8$ while the perplexity on the dev set is $815.9$.\n",
        "\n",
        "One observation from the results is that the bigram perplexity on the dev set is infinity. The bigram model has not seen some instances in the dev set as they are not present in the training set. Thus, the output for the corresponding count/probability will be 0. Consequently, the term $\\log_2 p(x^{(i)})$ will tend to negative infinity, causing the perplexity to become positive infinity.\n",
        "\n",
        "In the unigram language model, the perplexity on the dev set is slightly lower than the perplexity on the training set. This implies that the effective vocabulary of the model is lower in the dev set compared to the training set. This maybe caused by pure luck due to the combination of words and sentences present in the dev set. This model is able to more accurately predict the next word under the dev set compared to the training set.\n",
        "\n",
        "Comparing the perplexity on the training set for the bigram and unigram model, we can see that the bigram has a much lower perplexity than the unigram. This is because the assigned probabilities for a specific sentence in the bigram is higher, which causes the negative log-probability, thus the perplexity, to be lowered. Under the training set environment which is seen by the model, the bigram model is able to more accurately predict the next probable word than the unigram.\n",
        "\n",
        "It is also notable that, based on the experimental results, the bigram model is overfitting compared to the unigram model. From the results above, the bigram model achieves a low training perplexity but has an infinitely large validation perplexity, while the magnitude of the unigram perplexity is similar in both the training and dev set. This is because the bigram model exhausts all the combinatorial possibilities inside the vocabulary, which introduces the sparsity problem in the dataset containing the presence of each bigram for each sentence. As the bigram model is overfitting to the training set, it cannot generalize well to the development set, as compared to the unigram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "## 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93_yLu9dVr0C",
        "outputId": "24fda0cf-afd5-4a12-8c67-aa3e4e0b4060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-02-17 09:32:50.025905 training bigram model with add-one smoothing...\n",
            "Bigram perplexity on training set = 1442.5940705370795\n",
            "Bigram perplexity on dev set = 1660.2006059691935\n"
          ]
        }
      ],
      "source": [
        "# bigram model - training set\n",
        "bigram_text_training = []\n",
        "training_bigrams = []\n",
        "for sentence in sentences_training:\n",
        "    sentence = list(vocab.lookup(sentence))\n",
        "    bigram_pairs = list(bigrams(sentence))\n",
        "    bigram_text_training.append(bigram_pairs)\n",
        "    for bigram in bigram_pairs:\n",
        "        training_bigrams.append(bigram)\n",
        "bigram_model = Laplace(order=2, vocabulary=vocab)\n",
        "print(datetime.now(), 'training bigram model with add-one smoothing...')\n",
        "bigram_model.fit(bigram_text_training, vocab)\n",
        "training_perplexity = bigram_model.perplexity(training_bigrams)\n",
        "print(f'Bigram perplexity on training set = {training_perplexity}')\n",
        "# bigram model - development set\n",
        "dev_bigrams = []\n",
        "for sentence in sentences_dev:\n",
        "    sentence = list(vocab.lookup(sentence))\n",
        "    bigram_pairs = list(bigrams(sentence))\n",
        "    for bigram in bigram_pairs:\n",
        "        dev_bigrams.append(bigram)\n",
        "dev_perplexity = bigram_model.perplexity(dev_bigrams)\n",
        "print(f'Bigram perplexity on dev set = {dev_perplexity}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": [
        "The perplexity of the bigram model with add-one smoothing on the training set and dev set are $1442.6$ and $1660.2$ respectively.\n",
        "\n",
        "One difference in the perplexity with add-one smoothing compared to the previous part is that the bigram model is now less overfitting, as the perplexity on the training set and dev set is similar in magnitude.\n",
        "\n",
        "However, the bigram perplexity on the dev set decreases significantly (while that of the training set increases significantly). As bigrams with originally assigned probabilities of zero in the dev set are increased to the inverse of the vocabulary size, this introduces an uncertainty to the model as the cross-entropy decreases, which implies the predictions made by the bigram model is less random. This effectively lowers the perplexity in the dev set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "#### 1.3.2 Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jhcuJWo7VuNg"
      },
      "outputs": [],
      "source": [
        "def add_k_smoothing(k: float):\n",
        "    # bigram model - training set\n",
        "    bigram_text_training = []\n",
        "    training_bigrams = []\n",
        "    for sentence in sentences_training:\n",
        "        sentence = list(vocab.lookup(sentence))\n",
        "        bigram_pairs = list(bigrams(sentence))\n",
        "        bigram_text_training.append(bigram_pairs)\n",
        "        for bigram in bigram_pairs:\n",
        "            training_bigrams.append(bigram)\n",
        "    bigram_model = Lidstone(order=2, vocabulary=vocab, gamma=k)\n",
        "    print(datetime.now(), f'training bigram model with add-k smoothing, k={k}...')\n",
        "    bigram_model.fit(bigram_text_training, vocab)\n",
        "    training_perplexity = bigram_model.perplexity(training_bigrams)\n",
        "    print(f'Bigram perplexity on training set = {training_perplexity}')\n",
        "    # bigram model - development set\n",
        "    dev_bigrams = []\n",
        "    for sentence in sentences_dev:\n",
        "        sentence = list(vocab.lookup(sentence))\n",
        "        bigram_pairs = list(bigrams(sentence))\n",
        "        for bigram in bigram_pairs:\n",
        "            dev_bigrams.append(bigram)\n",
        "    dev_perplexity = bigram_model.perplexity(dev_bigrams)\n",
        "    print(f'Bigram perplexity on dev set = {dev_perplexity}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYy6Tdvo-Lnh",
        "outputId": "edd936af-9c0c-4f21-b41b-fd50b3e31787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-02-17 09:33:21.672733 training bigram model with add-k smoothing, k=0.01...\n",
            "Bigram perplexity on training set = 157.7053131615645\n",
            "Bigram perplexity on dev set = 437.5697758029117\n",
            "2024-02-17 09:33:54.535309 training bigram model with add-k smoothing, k=0.005...\n",
            "Bigram perplexity on training set = 129.34773785420956\n",
            "Bigram perplexity on dev set = 416.66775227938956\n",
            "2024-02-17 09:34:25.234471 training bigram model with add-k smoothing, k=0.001...\n",
            "Bigram perplexity on training set = 94.95746093615871\n",
            "Bigram perplexity on dev set = 432.42871956575476\n"
          ]
        }
      ],
      "source": [
        "Ks = [1e-2, 5e-3, 1e-3]\n",
        "for k in Ks:\n",
        "    add_k_smoothing(k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": [
        "In this optimization experiment, the values $k \\in \\{0.01, 0.005, 0.001\\}$ are chosen.\n",
        "\n",
        "For $k = 0.01$, the training and validation perplexity are $157.7$ and $437.6$ respectively.\n",
        "\n",
        "For $k = 0.005$, the training and validation perplexity are $129.3$ and $416.7$ respectively.\n",
        "\n",
        "For $k = 0.001$, the training and validation perplexity are $95.0$ and $432.4$ respectively.\n",
        "\n",
        "From these values of $k$, we can infer that the validation perplexity achieves a local minimum between $0.01$ and $0.001$, as the validation perplexity plotted against these values of $k$ exhibit a U-shaped curve. Thus, the optimized value of $k$ is chosen to be $0.005$.\n",
        "\n",
        "Compared to the add-one smoothing, both the training and validation perplexity decreases in the add-k smoothing bigram model. The difference in results between this sub-question and the previous sub-question is due to the probabilities assigned to unseen bigrams. Under add-k smoothing, the probabilities are only slightly altered, whereas the probabilities may be significantly changed under add-one smoothing.\n",
        "\n",
        "Another reason to explain why the add-k smoothing is better is because the value of $k$ can be alternatively viewed as the regularization strength of the objective function of the model. When $k$ increases, the strength of regularization increases which results in a simpler model, explaining the case where the training and validation perplexity is significantly larger in the case of add-one smoothing. As $k$ decreases, the regularization strength decreases which reduces the training perplexity. The validation perplexity, on the other hand, can generalize the best to the unseen bigrams when the strength of regularization is not as intense in the add-one smoothing case. This also justifies the use of a small value of $k$ in the add-k smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "### 1.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K4N_XuN6WVCQ"
      },
      "outputs": [],
      "source": [
        "# define a new class for the linear interpolation smoothing model\n",
        "class LinearInterpolation(LanguageModel):\n",
        "     \"\"\"Class for providing linear-interpolation smoothed scores.\n",
        "\n",
        "    In addition to the iniitalization arguments from BaseNgramModel,\n",
        "    it also requres lambda1, lambda2 and lambda3 to determine the weights for interpolation.\n",
        "    \"\"\"\n",
        "\n",
        "     def __init__(self,\n",
        "               lambda1: float, lambda2: float,\n",
        "               lambda3: Union[float, None],\n",
        "               *args, **kwargs):\n",
        "          super().__init__(order=3, *args, **kwargs)\n",
        "          self.order = 3\n",
        "          self.lambda1 = lambda1\n",
        "          self.lambda2 = lambda2\n",
        "          if lambda3 is None or (lambda1 + lambda2 + lambda3) != 1:\n",
        "               self.lambda3 = 1 - self.lambda1 - self.lambda2\n",
        "          else:\n",
        "               self.lambda3 = lambda3\n",
        "\n",
        "     def unmasked_score(self, word, context=None):\n",
        "          trigram_unmasked_score = self.context_counts(context).freq(word)\n",
        "          bigram_unmasked_score = self.context_counts((context[-1],)).freq(word)\n",
        "          unigram_unmasked_score = self.counts.unigrams.freq(word)\n",
        "          return (\n",
        "               self.lambda1 * trigram_unmasked_score +\n",
        "               self.lambda2 * bigram_unmasked_score +\n",
        "               self.lambda3 * unigram_unmasked_score\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_wnHRlHd-Lnj"
      },
      "outputs": [],
      "source": [
        "# prepare unigrams for training use\n",
        "unigram_text_training = []\n",
        "training_unigrams = []\n",
        "for sentence in sentences_training:\n",
        "    sentence = list(vocab.lookup(sentence))\n",
        "    unigrams = list(ngrams(sentence, n=1))\n",
        "    unigram_text_training.append(unigrams)\n",
        "    for unigram in unigrams:\n",
        "        training_unigrams.append(unigram)\n",
        "\n",
        "# prepare bigrams for training use\n",
        "bigram_text_training = []\n",
        "training_bigrams = []\n",
        "for sentence in sentences_training:\n",
        "    sentence = list(vocab.lookup(sentence))\n",
        "    bigram_pairs = list(bigrams(sentence))\n",
        "    bigram_text_training.append(bigram_pairs)\n",
        "    for bigram in bigram_pairs:\n",
        "        training_bigrams.append(bigram)\n",
        "\n",
        "# prepare trigrams for training use\n",
        "trigram_text_training = []\n",
        "training_trigrams = []\n",
        "for sentence in sentences_training:\n",
        "    sentence.insert(0, '<START>')\n",
        "    sentence = list(vocab.lookup(sentence))\n",
        "    trigram_pairs = list(trigrams(sentence))\n",
        "    trigram_text_training.append(trigram_pairs)\n",
        "    for trigram in trigram_pairs:\n",
        "        training_trigrams.append(trigram)\n",
        "\n",
        "# prepare trigrams for dev use\n",
        "dev_trigrams = []\n",
        "for sentence in sentences_dev:\n",
        "    sentence.insert(0, '<START>')\n",
        "    sentence = list(vocab.lookup(sentence))\n",
        "    trigram_pairs = list(trigrams(sentence))\n",
        "    for trigram in trigram_pairs:\n",
        "        dev_trigrams.append(trigram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qD_gzTd2-Lnj"
      },
      "outputs": [],
      "source": [
        "counter = NgramCounter(unigram_text_training + bigram_text_training + trigram_text_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "D-v-XHX_-Lnr"
      },
      "outputs": [],
      "source": [
        "def linear_interpolation_model(λ1: float, λ2: float, λ3: float):\n",
        "    interpolation_model = LinearInterpolation(lambda1=λ1, lambda2=λ2, lambda3=λ3,\n",
        "                                              vocabulary=vocab, counter=counter)\n",
        "    print(datetime.now(), f'training linear interpolation model with λ1={λ1}, λ2={λ2}, λ3={λ3}...')\n",
        "    interpolation_model.fit(trigram_text_training, vocab)\n",
        "    training_perplexity = interpolation_model.perplexity(training_trigrams)\n",
        "    print(f'perplexity on training set = {training_perplexity}')\n",
        "    dev_perplexity = interpolation_model.perplexity(dev_trigrams)\n",
        "    print(f'perplexity on dev set = {dev_perplexity}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgEDqQFz-Lnr",
        "outputId": "d418c8c4-cade-49e6-d05e-56c4e7e5fcd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-02-17 09:35:19.831368 training linear interpolation model with λ1=0.2, λ2=0.3, λ3=0.5...\n",
            "perplexity on training set = 25.107091102903894\n",
            "perplexity on dev set = 283.037409372196\n"
          ]
        }
      ],
      "source": [
        "# training on the linear interpolation model\n",
        "linear_interpolation_model(0.2, 0.3, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_eV0tPM-Lns",
        "outputId": "b0e6116c-5ced-4574-9ac9-be67d9d8b14f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-02-17 09:36:01.201851 training linear interpolation model with λ1=0.2, λ2=0.2, λ3=0.6...\n",
            "perplexity on training set = 26.861129817310378\n",
            "perplexity on dev set = 307.2068445979232\n",
            "2024-02-17 09:36:40.062359 training linear interpolation model with λ1=0.2, λ2=0.4, λ3=0.4...\n",
            "perplexity on training set = 23.664553497096755\n",
            "perplexity on dev set = 269.4067227510323\n",
            "2024-02-17 09:37:19.007396 training linear interpolation model with λ1=0.2, λ2=0.5, λ3=0.3...\n",
            "perplexity on training set = 22.447063943636785\n",
            "perplexity on dev set = 264.050881611073\n",
            "2024-02-17 09:37:58.713523 training linear interpolation model with λ1=0.2, λ2=0.6, λ3=0.2...\n",
            "perplexity on training set = 21.400428089985592\n",
            "perplexity on dev set = 268.32456638867234\n",
            "2024-02-17 09:38:38.479375 training linear interpolation model with λ1=0.1, λ2=0.6, λ3=0.3...\n",
            "perplexity on training set = 31.19881408077877\n",
            "perplexity on dev set = 262.8349689514711\n",
            "2024-02-17 09:39:17.658898 training linear interpolation model with λ1=0.1, λ2=0.5, λ3=0.4...\n",
            "perplexity on training set = 33.172476707171626\n",
            "perplexity on dev set = 266.5963023104589\n",
            "2024-02-17 09:39:57.604863 training linear interpolation model with λ1=0.1, λ2=0.4, λ3=0.5...\n",
            "perplexity on training set = 35.541365187127866\n",
            "perplexity on dev set = 277.5209390203025\n",
            "2024-02-17 09:40:36.196306 training linear interpolation model with λ1=0.1, λ2=0.3, λ3=0.6...\n",
            "perplexity on training set = 38.462738422274334\n",
            "perplexity on dev set = 296.4599435590473\n"
          ]
        }
      ],
      "source": [
        "# optimization\n",
        "lambdas_list = [\n",
        "    (0.2, 0.2, 0.6),\n",
        "    (0.2, 0.4, 0.4),\n",
        "    (0.2, 0.5, 0.3),\n",
        "    (0.2, 0.6, 0.2),\n",
        "    (0.1, 0.6, 0.3),\n",
        "    (0.1, 0.5, 0.4),\n",
        "    (0.1, 0.4, 0.5),\n",
        "    (0.1, 0.3, 0.6)\n",
        "]\n",
        "for lambdas in lambdas_list:\n",
        "    λ1, λ2, λ3 = lambdas\n",
        "    linear_interpolation_model(λ1, λ2, λ3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gycvrOz6-Lnt"
      },
      "outputs": [],
      "source": [
        "# prepare the test sentences and trigrams\n",
        "# read the test.txt file\n",
        "with open('data/lm/test.txt', 'r') as file:\n",
        "    sentences_test = file.readlines()\n",
        "    for i, text in enumerate(sentences_test):\n",
        "        text_list = text.split()\n",
        "        text_list.insert(0, '<START>')\n",
        "        text_list.append('<END>')\n",
        "        sentences_test[i] = text_list\n",
        "\n",
        "# prepare trigrams for test use\n",
        "test_trigrams = []\n",
        "for sentence in sentences_test:\n",
        "    sentence.insert(0, '<START>')\n",
        "    sentence = list(vocab.lookup(sentence))\n",
        "    trigram_pairs = list(trigrams(sentence))\n",
        "    for trigram in trigram_pairs:\n",
        "        test_trigrams.append(trigram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxCvFCYi-Lnt",
        "outputId": "c1d98550-daa9-47bc-c087-18a842feec57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-02-17 09:41:15.810189 training linear interpolation model with λ1=0.1, λ2=0.6, λ3=0.3...\n",
            "perplexity on test set = 262.35175842315573\n"
          ]
        }
      ],
      "source": [
        "# fit the best optimized model on the test set\n",
        "λ1, λ2, λ3 = 0.1, 0.6, 0.3\n",
        "interpolation_model = LinearInterpolation(lambda1=λ1, lambda2=λ2, lambda3=λ3,\n",
        "                                          vocabulary=vocab, counter=counter)\n",
        "print(datetime.now(), f'training linear interpolation model with λ1={λ1}, λ2={λ2}, λ3={λ3}...')\n",
        "interpolation_model.fit(trigram_text_training, vocab)\n",
        "test_perplexity = interpolation_model.perplexity(test_trigrams)\n",
        "print(f'perplexity on test set = {test_perplexity}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": [
        "For $\\lambda_1 = 0.2$, $\\lambda_2 = 0.3$, $\\lambda_3= 0.5$, the perplexity on the training set and dev set are $25.1$ and $283.0$ respectively.\n",
        "\n",
        "By optimizing the dev set on the hyperparameter sets $(\\lambda_1, \\lambda_2, \\lambda_3) \\in \\{(0.2, 0.2, 0.6), (0.2, 0.4, 0,4), (0.2, 0.5, 0.3), (0.2, 0.6, 0.2), $\n",
        "$(0.1, 0.6, 0.3), (0.1, 0.5, 0.4), (0.1, 0.4, 0.5), (0.1, 0.3, 0.6) \\}$, the best hyperparameter set which minimizes the perplexity in the dev set is $(\\lambda_1, \\lambda_2, \\lambda_3) = (0.1, 0.6, 0.3)$, with a training perplexity of $31.2$ and a perplexity of $262.8$ on the dev set.\n",
        "\n",
        "The perplexity on the test set with the best hyperparameter set $(\\lambda_1, \\lambda_2, \\lambda_3) = (0.1, 0.6, 0.3)$ is $262.4$.\n",
        "\n",
        "The linear interpolation smoothing model performs much better than the add-k smoothing language model, as suggested by the decrease in both the perplexity on the training and dev set. In the previous sub-questions, the add-k smoothing is only performed on the bigram model, whereas the linear interpolation smoothing is performed between unigram, bigram and trigram models, which mitigates some deficiencies in the previous model.\n",
        "\n",
        "The linear interpolation smoothing model can better capture the semantics inside each sentence. As some English phrases have three or more words, incorporating trigrams can capture these phrases, so that the model can recognize them through the trigram score given the previous context of the two words.\n",
        "\n",
        "The optimal weights $(\\lambda_1, \\lambda_2, \\lambda_3) = (0.1, 0.6, 0.3)$ suggest that a larger weight should be put on the bigram model, followed by the unigram and trigram model, meaning that bigram features contributes to a higher score, thus a lower perplexity of the model. The small weight on the trigram score captures the score contributed by the previous two words. The weight on the unigram score act as a backup to avoid the perplexity to become infinity, in case a word combination is not found in the training bigrams or trigrams.\n",
        "\n",
        "From the experimental results above, we can see that the optimized linear interpolation smoothing model is not overfitting, as the perplexity on the test set is of similar magnitude compared to the one on the dev set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### 1.3.4 Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": [
        "Bayesian Optimization is an example of a learning algorithm to find the optimal set of hyperparameters.\n",
        "\n",
        "Bayesian Optimization is an optimization technique which aims to maximize or minimize the objective function (for instance, language model perplexity in this question). This technique takes a probabilistic approach to optimize a set of hyperparameters, keeping a memory of past hyperparameter combinations which can maximize the score/value of the objective function. The optimization technique starts with a prior probability distribution where we assume that all sets of hyperparameters perform equally well. After sufficient tuning, the posterior distribution will be updated based on different sets of hyperparameters, thus estimating the true form of the objective function well. Compared to traditional learning methods like grid search and randomized search, Bayesian Optimization has the advantage of not assuming the functional form of the objective function. By using a probabilistic approach, the effort in tuning the set of hyperparameters is lowered, as less unnecessary calls to the objective function is made."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPBLm280-Lnv"
      },
      "source": [
        "# 2 Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnn2FsHK-Lnv",
        "outputId": "a177b888-e85e-4ae3-82d9-8d6ff2f3e577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vocab size 400000\n"
          ]
        }
      ],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each length 50\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
        "    return wv_from_bin\n",
        "\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZVaqGgw-Lnv"
      },
      "source": [
        "## 2.1 Find most similar word\n",
        "Use cosine similarity to find the most similar word to each of these words. Report the most similar word and its cosine similarity.\n",
        "\n",
        "* dog\n",
        "* whale\n",
        "* before\n",
        "* however\n",
        "* fabricate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9YnNjIh-Lnw",
        "outputId": "437e9240-b145-495c-8aea-913bb8e084c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most similar to \"dog\" is \"dogs\" with cosine similarity 0.8136862516403198\n",
            "The most similar to \"whale\" is \"whales\" with cosine similarity 0.7918056845664978\n",
            "The most similar to \"before\" is \"after\" with cosine similarity 0.8931248188018799\n",
            "The most similar to \"however\" is \"although\" with cosine similarity 0.9336162805557251\n",
            "The most similar to \"fabricate\" is \"fabricating\" with cosine similarity 0.618401825428009\n"
          ]
        }
      ],
      "source": [
        "words = ['dog', 'whale', 'before', 'however', 'fabricate']\n",
        "for word in words:\n",
        "    most_similar_word, similarity = wv_from_bin.most_similar(positive=word, topn=1)[0]\n",
        "    print(f'The most similar to \"{word}\" is \"{most_similar_word}\" with cosine similarity {similarity}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV_uKH1z-Lnw"
      },
      "source": [
        "## 2.2 Finding Analogies\n",
        "Use vector addition and subtraction to compute target vectors for the analogies below. After computing each target vector, find the top three candidates by cosine similarity. Report the candidates and their similarities to the target vector.\n",
        "\n",
        "- dog : puppy :: cat : ?:\n",
        "- speak : speaker :: sing : ?:\n",
        "- France : French :: England : ?:\n",
        "- France : wine :: England : ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iONDF2jz-Lnw",
        "outputId": "cf55fb9a-e7e1-48a0-c79e-8014e8c706d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For the analogy dog:puppy::cat:?, the top 3 candidate words with their similarities are: \n",
            "1. puppies, similarity = 0.6142244935035706\n",
            "2. kitten, similarity = 0.5919069647789001\n",
            "3. kittens, similarity = 0.5758378505706787\n",
            "----\n",
            "For the analogy speak:speaker::sing:?, the top 3 candidate words with their similarities are: \n",
            "1. sang, similarity = 0.48609817028045654\n",
            "2. chorus, similarity = 0.4438377618789673\n",
            "3. singing, similarity = 0.4332594871520996\n",
            "----\n",
            "For the analogy France:French::England:?, the top 3 candidate words with their similarities are: \n",
            "1. english, similarity = 0.7599895596504211\n",
            "2. british, similarity = 0.5879749059677124\n",
            "3. scottish, similarity = 0.5616408586502075\n",
            "----\n",
            "For the analogy France:wine::England:?, the top 3 candidate words with their similarities are: \n",
            "1. tea, similarity = 0.540773868560791\n",
            "2. wines, similarity = 0.5329136848449707\n",
            "3. yorkshire, similarity = 0.49405309557914734\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "analogies = [\n",
        "    'dog:puppy::cat:?',\n",
        "    'speak:speaker::sing:?',\n",
        "    'France:French::England:?',\n",
        "    'France:wine::England:?'\n",
        "]\n",
        "for analogy in analogies:\n",
        "    word_list = re.split(r':+', analogy)\n",
        "    w1, w2, w3 = word_list[0].lower(), word_list[1].lower(), word_list[2].lower()\n",
        "    results = wv_from_bin.most_similar(positive=[w3, w2], negative=[w1], topn=3)\n",
        "    print(f'For the analogy {analogy}, the top 3 candidate words with their similarities are: ')\n",
        "    for i, result in enumerate(results):\n",
        "        candidate_word, similarity = result\n",
        "        print(f'{i+1}. {candidate_word}, similarity = {similarity}')\n",
        "    print('----')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJJH6bTN-Lnx"
      },
      "source": [
        "# 3 Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx__OLSK-Lnx",
        "outputId": "4e436de5-cb81-4811-c6c4-d4963c22360c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-17 09:42:46--  https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 738844 (722K) [text/plain]\n",
            "Saving to: ‘data/classification/train.txt’\n",
            "\n",
            "\r          data/clas   0%[                    ]       0  --.-KB/s               \rdata/classification 100%[===================>] 721.53K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-02-17 09:42:46 (15.7 MB/s) - ‘data/classification/train.txt’ saved [738844/738844]\n",
            "\n",
            "--2024-02-17 09:42:46--  https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94400 (92K) [text/plain]\n",
            "Saving to: ‘data/classification/dev.txt’\n",
            "\n",
            "data/classification 100%[===================>]  92.19K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-02-17 09:42:46 (6.16 MB/s) - ‘data/classification/dev.txt’ saved [94400/94400]\n",
            "\n",
            "--2024-02-17 09:42:46--  https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/test-blind.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 189920 (185K) [text/plain]\n",
            "Saving to: ‘data/classification/test-blind.txt’\n",
            "\n",
            "data/classification 100%[===================>] 185.47K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-02-17 09:42:46 (7.34 MB/s) - ‘data/classification/test-blind.txt’ saved [189920/189920]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/classification\n",
        "!wget -O data/classification/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/train.txt\n",
        "!wget -O data/classification/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/dev.txt\n",
        "!wget -O data/classification/test-blind.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/test-blind.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFaYykb0-Lnx"
      },
      "source": [
        "## 3.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pQwci-Am-Lnx"
      },
      "outputs": [],
      "source": [
        "PATTERN = re.compile(r'([0-1])\\t(.*)')\n",
        "\n",
        "# read the dataset and preprocess it before model fitting\n",
        "train_sentences = []\n",
        "train_labels = []\n",
        "# read the train.txt file\n",
        "with open('data/classification/train.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for i, text in enumerate(lines):\n",
        "        match = re.search(PATTERN, text)\n",
        "        label, sentence = int(match.group(1)), match.group(2)\n",
        "        train_sentences.append(sentence)\n",
        "        train_labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "R_IYYIw8-Lnx"
      },
      "outputs": [],
      "source": [
        "dev_sentences = []\n",
        "dev_labels = []\n",
        "# read the dev.txt file\n",
        "with open('data/classification/dev.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for i, text in enumerate(lines):\n",
        "        match = re.search(PATTERN, text)\n",
        "        label, sentence = int(match.group(1)), match.group(2)\n",
        "        dev_sentences.append(sentence)\n",
        "        dev_labels.append(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMl1Cw_k-Lny"
      },
      "source": [
        "### Unigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu0xTCCO-Lny",
        "outputId": "46eef795-f080-4c65-ea62-7c7164e747e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision = 0.7866666666666666\n",
            "Recall = 0.7972972972972973\n",
            "F1-score = 0.7919463087248321\n"
          ]
        }
      ],
      "source": [
        "unigram_vec = CountVectorizer()\n",
        "X_train = unigram_vec.fit_transform(train_sentences)\n",
        "y_train = np.array(train_labels)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=MAX_ITER, random_state=RANDOM_STATE)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_dev = unigram_vec.transform(dev_sentences)\n",
        "y_pred = model.predict(X_dev)\n",
        "y_dev = np.array(dev_labels)\n",
        "\n",
        "precision = precision_score(y_dev, y_pred)\n",
        "recall = recall_score(y_dev, y_pred)\n",
        "f1 = f1_score(y_dev, y_pred)\n",
        "\n",
        "print(f'Precision = {precision}')\n",
        "print(f'Recall = {recall}')\n",
        "print(f'F1-score = {f1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g48e0_X-Lny"
      },
      "source": [
        "### Bigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3otllhPn-Lny",
        "outputId": "db41e8f6-8f08-44c1-bf89-84cb66d62ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision = 0.7104166666666667\n",
            "Recall = 0.7680180180180181\n",
            "F1-score = 0.7380952380952381\n"
          ]
        }
      ],
      "source": [
        "bigram_vec = CountVectorizer(ngram_range=(2, 2))\n",
        "X_train = bigram_vec.fit_transform(train_sentences)\n",
        "y_train = np.array(train_labels)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=MAX_ITER, random_state=RANDOM_STATE)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_dev = bigram_vec.transform(dev_sentences)\n",
        "y_pred = model.predict(X_dev)\n",
        "y_dev = np.array(dev_labels)\n",
        "\n",
        "precision = precision_score(y_dev, y_pred)\n",
        "recall = recall_score(y_dev, y_pred)\n",
        "f1 = f1_score(y_dev, y_pred)\n",
        "\n",
        "print(f'Precision = {precision}')\n",
        "print(f'Recall = {recall}')\n",
        "print(f'F1-score = {f1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNE7d5Aw-Lnz"
      },
      "source": [
        "### GloVe Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gUcTGHS7-Lnz"
      },
      "outputs": [],
      "source": [
        "# assume GloVe features = average of the word vectors in a sentence\n",
        "def get_glove_features(sentence: str) -> np.array:\n",
        "    mean_vector = wv_from_bin.get_mean_vector([word.lower() for word in sentence.split()])\n",
        "    return mean_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4HotDlb-Lnz",
        "outputId": "4e48bbdc-99b8-4196-e75d-07a9eb37fe32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision = 0.7709251101321586\n",
            "Recall = 0.7882882882882883\n",
            "F1-score = 0.7795100222717148\n"
          ]
        }
      ],
      "source": [
        "X_train = np.array([get_glove_features(sent) for sent in train_sentences])\n",
        "y_train = np.array(train_labels)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=MAX_ITER, random_state=RANDOM_STATE)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_dev = np.array([get_glove_features(sent) for sent in dev_sentences])\n",
        "y_pred = model.predict(X_dev)\n",
        "y_dev = np.array(dev_labels)\n",
        "\n",
        "precision = precision_score(y_dev, y_pred)\n",
        "recall = recall_score(y_dev, y_pred)\n",
        "f1 = f1_score(y_dev, y_pred)\n",
        "\n",
        "print(f'Precision = {precision}')\n",
        "print(f'Recall = {recall}')\n",
        "print(f'F1-score = {f1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWMArvRQ-Lnz"
      },
      "source": [
        "Compare the performance of three types of features on dev set. Report the weighted average precision, recall and F1-score for each feature set.\n",
        "\n",
        "| Feature | precision | recall | F1-score |\n",
        "| ----------- | --------- | ------ | -------- |\n",
        "| unigram     | 0.7867          | 0.7973       | 0.7919         |\n",
        "| bigram      | 0.7104          | 0.7680       | 0.7381         |\n",
        "| GloVe       | 0.7709          | 0.7883       | 0.7795         |\n",
        "\n",
        "Overall, the performance for unigram features is the highest, followed by GloVe features, then bigram features.\n",
        "\n",
        "All three features have the recall score greater than the precision score, which suggests that the logistic regression model has identified fewer false negatives than false positives.\n",
        "\n",
        "One reason to account for the poor performance for the model with bigram features (as compared to other features) is that the feature matrix of the bigram features is very sparse. Based on the training corpus of sentences, there are a total of more than $67000$ bigram features. As there are only a few words in each sentence inside the training example, we would expect only 20-30 of these features having a value of `1`, with the other features having a value `0`. Thus, we may not have enough samples to train on specific bigram features, considering that there are tens of thousands of bigrams.\n",
        "\n",
        "On the other hand, the model with unigram features slightly exceeds the performance of GloVe features, which suggests that the count and occurrence of each word is slightly better in correctly identifying positive samples, compared to the global representation/semantic meaning of each sentence as used in GloVe features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yklrDXnL-Ln0"
      },
      "source": [
        "## 3.2 Better Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-wQlFRJzjpj"
      },
      "source": [
        "In this sub-question, the following feature modifications are performed on the unigram/bigram model:\n",
        "* Create a new feature which captures the count of negation words in a sentence (e.g. not, never and none).\n",
        "* Create a new feature which captures the count of conjunctions in a sentence (e.g. but, yet, however).\n",
        "* Create a new feature which captures the count of positive adjectives in a sentence, randomly sampled from the training dataset.\n",
        "* Create a new feature which captures the count of negative adjectives in a sentence, randomly sampled from the training dataset.\n",
        "* Include trigrams into consideration.\n",
        "* Use a subset of features taking from unigrams, bigrams and trigrams.\n",
        "* Use tf-idf weighting instead of the regular weighting using Maximum Likelihood Estimation (MLE).\n",
        "* Disregard commonly appearing words which appear more than $n$ times in the training dataset, where $n \\in \\{200, 500, 800, 1000, 1200, 1500, 2000, 3000\\}$.\n",
        "* Disregard rare words which appear less than $n$ times in the training dataset, where $n \\in \\{2, 3, 4, 5, 10, 20, 50\\}$.\n",
        "* Disregard English stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5smWOODW-Ln0"
      },
      "outputs": [],
      "source": [
        "# a new feature is created which captures the common negation words in English\n",
        "NEGATION_WORDS_PATTERN = re.compile(r'\\b(n(\\'|o)t|never|nor|none|nothing|nowhere)\\b', flags=re.IGNORECASE)\n",
        "\n",
        "def get_negation_words_count(sentence: str) -> int:\n",
        "    matches = re.findall(NEGATION_WORDS_PATTERN, sentence)\n",
        "    return len(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7ZXs-9ii-Ln0"
      },
      "outputs": [],
      "source": [
        "# a new feature is created which captures the common conjunctions with convey opposite meaning in English\n",
        "CONJUNCTIONS_PATTERN = re.compile(r'\\b(but|yet|however|(al)?though|(never|none)theless)\\b', flags=re.IGNORECASE)\n",
        "\n",
        "def get_conjunction_words_count(sentence: str) -> int:\n",
        "    matches = re.findall(CONJUNCTIONS_PATTERN, sentence)\n",
        "    return len(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "u8XEJCs6-Ln0"
      },
      "outputs": [],
      "source": [
        "# a new feature is created to capture the occurrence of positive adjectives\n",
        "POSITIVE_ADJ_PATTERN = re.compile(r'\\b(charming|fascinating|astonishing|good|better|best|excellent|terrific|fantastic|vivid(ly)?|nice|expressive|funn(y|ier|iest)|beautiful|appealing|sophisticated|sweet|winning|fascinating|happy|glorious|inspiring|impressive|intriguing)\\b', flags=re.IGNORECASE)\n",
        "\n",
        "def get_positive_adj_count(sentence: str) -> int:\n",
        "    matches = re.findall(POSITIVE_ADJ_PATTERN, sentence)\n",
        "    return len(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "d-VJrqKS-Ln1"
      },
      "outputs": [],
      "source": [
        "# a new feature is created to capture the occurrence of negative adjectives\n",
        "NEGATIVE_ADJ_PATTERN = re.compile(r'\\b(bad|worse|worst|horrible|silly|unoriginal|loose|imitation(s)?|clich(e|é)s|mediocre|bleak|incoheren(t|ce)|lack|odd(s)?|empty|evil|dull|predictable|bland|mess(y)?|disappoint(ed|ment)|dishonest|unappealing|awful|choppy|pointless|idiotic|ill(\\-[a-z]+)?)\\b', flags=re.IGNORECASE)\n",
        "\n",
        "def get_negative_adj_count(sentence: str) -> int:\n",
        "    matches = re.findall(NEGATIVE_ADJ_PATTERN, sentence)\n",
        "    return len(matches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHudZGoj16l2"
      },
      "source": [
        "After a series of optimization and fitting the transformed data to the training dataset (and the dev dataset for evaluation), the following transformations are applied to the original dataset:\n",
        "* Create a new feature which captures the count of conjunctions in a sentence (e.g. but, yet, however).\n",
        "* Create a new feature which captures the count of positive adjectives in a sentence, randomly sampled from the training dataset.\n",
        "* Create a new feature which captures the count of negative adjectives in a sentence, randomly sampled from the training dataset.\n",
        "* Use both unigrams and bigrams as features.\n",
        "* Use tf-idf weighting.\n",
        "* Disregard commonly appearing words which appear more than $1000$ times in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iY4oTD-X-Ln1"
      },
      "outputs": [],
      "source": [
        "def apply_transformation_to_sentences(sentences: List,\n",
        "                                      vec: Union[CountVectorizer, TfidfVectorizer],\n",
        "                                      train: bool = True) -> np.array:\n",
        "    if train:\n",
        "        transformed_feats = vec.fit_transform(sentences)\n",
        "    else:\n",
        "        transformed_feats = vec.transform(sentences)\n",
        "    # negation_words_feature = np.array([get_negation_words_count(sentence) for sentence in sentences])\n",
        "    conjunction_words_feature = np.array([get_conjunction_words_count(sentence) for sentence in sentences])\n",
        "    positive_adj_feature = np.array([get_positive_adj_count(sentence) for sentence in sentences])\n",
        "    negative_adj_feature = np.array([get_negative_adj_count(sentence) for sentence in sentences])\n",
        "    result = np.hstack((\n",
        "                        transformed_feats.toarray(),\n",
        "                        # negation_words_feature[:, np.newaxis],\n",
        "                        conjunction_words_feature[:, np.newaxis],\n",
        "                        positive_adj_feature[:, np.newaxis],\n",
        "                        negative_adj_feature[:, np.newaxis],\n",
        "                        ))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4qR9tqZ-Ln1",
        "outputId": "dc9c3b06-8b24-42a1-c18d-26140693932a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision = 0.8070953436807096\n",
            "Recall = 0.8198198198198198\n",
            "F1-score = 0.8134078212290502\n"
          ]
        }
      ],
      "source": [
        "# use unigram\n",
        "vec = TfidfVectorizer(ngram_range=(1, 2), max_df=1000)\n",
        "X_train = apply_transformation_to_sentences(train_sentences, vec)\n",
        "y_train = np.array(train_labels)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=MAX_ITER, random_state=RANDOM_STATE)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_dev = apply_transformation_to_sentences(dev_sentences, vec, train=False)\n",
        "y_pred = model.predict(X_dev)\n",
        "y_dev = np.array(dev_labels)\n",
        "\n",
        "precision = precision_score(y_dev, y_pred)\n",
        "recall = recall_score(y_dev, y_pred)\n",
        "f1 = f1_score(y_dev, y_pred)\n",
        "\n",
        "print(f'Precision = {precision}')\n",
        "print(f'Recall = {recall}')\n",
        "print(f'F1-score = {f1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwiwKYha2qFo"
      },
      "source": [
        "Based on the transformations applied, the performance on the dev set which the feature modifications has slightly improved. The precision, recall and f1-score obtained are $0.8071$, $0.8198$ and $0.8134$ respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW36piyh3Zj5"
      },
      "source": [
        "Finally, this model is applied to the test set and an output text file is generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "7EDWSmXR-Ln2"
      },
      "outputs": [],
      "source": [
        "# finally, read the test sentences and generate the predictions\n",
        "test_sentences = []\n",
        "test_labels = []\n",
        "# read the dev.txt file\n",
        "with open('data/classification/test-blind.txt', 'r') as file:\n",
        "    test_sentences = file.readlines()\n",
        "X_test = apply_transformation_to_sentences(test_sentences, vec, train=False)\n",
        "y_pred = model.predict(X_test)\n",
        "with open(f'{UID}.test.txt', 'w') as file:\n",
        "    for i, sentence in enumerate(test_sentences):\n",
        "        label = y_pred[i]\n",
        "        file.write(f'{label}\\t{sentence}')\n",
        "    file.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
